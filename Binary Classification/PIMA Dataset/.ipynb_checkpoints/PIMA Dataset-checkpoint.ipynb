{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2349,
     "status": "ok",
     "timestamp": 1541940310028,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "-9dukIa7xIRh",
    "outputId": "d4bf7540-3d63-4134-d4a8-fa316d0a237a"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from google.colab import files\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from numpy import genfromtxt\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWPoSJkQfrTs"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5700,
     "status": "ok",
     "timestamp": 1541940317815,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "jYZp2y4oxVZH",
    "outputId": "0a90d979-9520-4a5a-f675-d9615835af2b"
   },
   "outputs": [],
   "source": [
    "# For Google Collab\n",
    "#files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UO-MblpLy1uA"
   },
   "outputs": [],
   "source": [
    "# Doing it via pd\n",
    "dataset = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1969
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1421,
     "status": "ok",
     "timestamp": 1541940320472,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "zcjVLdEly5QW",
    "outputId": "ba0067c2-4fa4-4896-d686-ab663be5f336"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>116</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.201</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.3</td>\n",
       "      <td>0.134</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>125</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.191</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>168</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.537</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.1</td>\n",
       "      <td>1.441</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>25.8</td>\n",
       "      <td>0.587</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>45.8</td>\n",
       "      <td>0.551</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>107</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>96</td>\n",
       "      <td>34.6</td>\n",
       "      <td>0.529</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>126</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>235</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.704</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>99</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.388</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7</td>\n",
       "      <td>196</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>80</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.263</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11</td>\n",
       "      <td>143</td>\n",
       "      <td>94</td>\n",
       "      <td>33</td>\n",
       "      <td>146</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.254</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>125</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>115</td>\n",
       "      <td>31.1</td>\n",
       "      <td>0.205</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7</td>\n",
       "      <td>147</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.257</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>140</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0.487</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13</td>\n",
       "      <td>145</td>\n",
       "      <td>82</td>\n",
       "      <td>19</td>\n",
       "      <td>110</td>\n",
       "      <td>22.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>117</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.1</td>\n",
       "      <td>0.337</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>160</td>\n",
       "      <td>36.6</td>\n",
       "      <td>0.453</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.293</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>37</td>\n",
       "      <td>150</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "      <td>44</td>\n",
       "      <td>20</td>\n",
       "      <td>94</td>\n",
       "      <td>30.8</td>\n",
       "      <td>0.400</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>58</td>\n",
       "      <td>18</td>\n",
       "      <td>116</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.219</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.734</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>140</td>\n",
       "      <td>40.6</td>\n",
       "      <td>1.174</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>84</td>\n",
       "      <td>33</td>\n",
       "      <td>105</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>1</td>\n",
       "      <td>147</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>49.3</td>\n",
       "      <td>0.358</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>57</td>\n",
       "      <td>46.3</td>\n",
       "      <td>1.096</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>3</td>\n",
       "      <td>187</td>\n",
       "      <td>70</td>\n",
       "      <td>22</td>\n",
       "      <td>200</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.408</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>6</td>\n",
       "      <td>162</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0.178</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.2</td>\n",
       "      <td>1.182</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>1</td>\n",
       "      <td>121</td>\n",
       "      <td>78</td>\n",
       "      <td>39</td>\n",
       "      <td>74</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.261</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>3</td>\n",
       "      <td>108</td>\n",
       "      <td>62</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.223</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0</td>\n",
       "      <td>181</td>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>510</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.222</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>8</td>\n",
       "      <td>154</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.443</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>110</td>\n",
       "      <td>36.5</td>\n",
       "      <td>1.057</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>7</td>\n",
       "      <td>137</td>\n",
       "      <td>90</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.391</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.3</td>\n",
       "      <td>0.258</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.197</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>6</td>\n",
       "      <td>190</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>0.278</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>2</td>\n",
       "      <td>88</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.766</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>9</td>\n",
       "      <td>170</td>\n",
       "      <td>74</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.403</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>9</td>\n",
       "      <td>89</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.142</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>180</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>70</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5</td>\n",
       "      <td>121</td>\n",
       "      <td>72</td>\n",
       "      <td>23</td>\n",
       "      <td>112</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>70</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0              6      148             72             35        0  33.6   \n",
       "1              1       85             66             29        0  26.6   \n",
       "2              8      183             64              0        0  23.3   \n",
       "3              1       89             66             23       94  28.1   \n",
       "4              0      137             40             35      168  43.1   \n",
       "5              5      116             74              0        0  25.6   \n",
       "6              3       78             50             32       88  31.0   \n",
       "7             10      115              0              0        0  35.3   \n",
       "8              2      197             70             45      543  30.5   \n",
       "9              8      125             96              0        0   0.0   \n",
       "10             4      110             92              0        0  37.6   \n",
       "11            10      168             74              0        0  38.0   \n",
       "12            10      139             80              0        0  27.1   \n",
       "13             1      189             60             23      846  30.1   \n",
       "14             5      166             72             19      175  25.8   \n",
       "15             7      100              0              0        0  30.0   \n",
       "16             0      118             84             47      230  45.8   \n",
       "17             7      107             74              0        0  29.6   \n",
       "18             1      103             30             38       83  43.3   \n",
       "19             1      115             70             30       96  34.6   \n",
       "20             3      126             88             41      235  39.3   \n",
       "21             8       99             84              0        0  35.4   \n",
       "22             7      196             90              0        0  39.8   \n",
       "23             9      119             80             35        0  29.0   \n",
       "24            11      143             94             33      146  36.6   \n",
       "25            10      125             70             26      115  31.1   \n",
       "26             7      147             76              0        0  39.4   \n",
       "27             1       97             66             15      140  23.2   \n",
       "28            13      145             82             19      110  22.2   \n",
       "29             5      117             92              0        0  34.1   \n",
       "..           ...      ...            ...            ...      ...   ...   \n",
       "738            2       99             60             17      160  36.6   \n",
       "739            1      102             74              0        0  39.5   \n",
       "740           11      120             80             37      150  42.3   \n",
       "741            3      102             44             20       94  30.8   \n",
       "742            1      109             58             18      116  28.5   \n",
       "743            9      140             94              0        0  32.7   \n",
       "744           13      153             88             37      140  40.6   \n",
       "745           12      100             84             33      105  30.0   \n",
       "746            1      147             94             41        0  49.3   \n",
       "747            1       81             74             41       57  46.3   \n",
       "748            3      187             70             22      200  36.4   \n",
       "749            6      162             62              0        0  24.3   \n",
       "750            4      136             70              0        0  31.2   \n",
       "751            1      121             78             39       74  39.0   \n",
       "752            3      108             62             24        0  26.0   \n",
       "753            0      181             88             44      510  43.3   \n",
       "754            8      154             78             32        0  32.4   \n",
       "755            1      128             88             39      110  36.5   \n",
       "756            7      137             90             41        0  32.0   \n",
       "757            0      123             72              0        0  36.3   \n",
       "758            1      106             76              0        0  37.5   \n",
       "759            6      190             92              0        0  35.5   \n",
       "760            2       88             58             26       16  28.4   \n",
       "761            9      170             74             31        0  44.0   \n",
       "762            9       89             62              0        0  22.5   \n",
       "763           10      101             76             48      180  32.9   \n",
       "764            2      122             70             27        0  36.8   \n",
       "765            5      121             72             23      112  26.2   \n",
       "766            1      126             60              0        0  30.1   \n",
       "767            1       93             70             31        0  30.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                       0.627   50        1  \n",
       "1                       0.351   31        0  \n",
       "2                       0.672   32        1  \n",
       "3                       0.167   21        0  \n",
       "4                       2.288   33        1  \n",
       "5                       0.201   30        0  \n",
       "6                       0.248   26        1  \n",
       "7                       0.134   29        0  \n",
       "8                       0.158   53        1  \n",
       "9                       0.232   54        1  \n",
       "10                      0.191   30        0  \n",
       "11                      0.537   34        1  \n",
       "12                      1.441   57        0  \n",
       "13                      0.398   59        1  \n",
       "14                      0.587   51        1  \n",
       "15                      0.484   32        1  \n",
       "16                      0.551   31        1  \n",
       "17                      0.254   31        1  \n",
       "18                      0.183   33        0  \n",
       "19                      0.529   32        1  \n",
       "20                      0.704   27        0  \n",
       "21                      0.388   50        0  \n",
       "22                      0.451   41        1  \n",
       "23                      0.263   29        1  \n",
       "24                      0.254   51        1  \n",
       "25                      0.205   41        1  \n",
       "26                      0.257   43        1  \n",
       "27                      0.487   22        0  \n",
       "28                      0.245   57        0  \n",
       "29                      0.337   38        0  \n",
       "..                        ...  ...      ...  \n",
       "738                     0.453   21        0  \n",
       "739                     0.293   42        1  \n",
       "740                     0.785   48        1  \n",
       "741                     0.400   26        0  \n",
       "742                     0.219   22        0  \n",
       "743                     0.734   45        1  \n",
       "744                     1.174   39        0  \n",
       "745                     0.488   46        0  \n",
       "746                     0.358   27        1  \n",
       "747                     1.096   32        0  \n",
       "748                     0.408   36        1  \n",
       "749                     0.178   50        1  \n",
       "750                     1.182   22        1  \n",
       "751                     0.261   28        0  \n",
       "752                     0.223   25        0  \n",
       "753                     0.222   26        1  \n",
       "754                     0.443   45        1  \n",
       "755                     1.057   37        1  \n",
       "756                     0.391   39        0  \n",
       "757                     0.258   52        1  \n",
       "758                     0.197   26        0  \n",
       "759                     0.278   66        1  \n",
       "760                     0.766   22        0  \n",
       "761                     0.403   43        1  \n",
       "762                     0.142   33        0  \n",
       "763                     0.171   63        0  \n",
       "764                     0.340   27        0  \n",
       "765                     0.245   30        0  \n",
       "766                     0.349   47        1  \n",
       "767                     0.315   23        0  \n",
       "\n",
       "[768 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kslfj-0Qz3Ki"
   },
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=['Outcome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VFYp2EsE0T2c"
   },
   "outputs": [],
   "source": [
    "Y = dataset[dataset.columns[8:9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeqfrKJ31jyK"
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(X)\n",
    "X = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ncJ70nKu3uuf"
   },
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDUIfT8-4ptA"
   },
   "outputs": [],
   "source": [
    "# X_train = X_train.values\n",
    "# X_test = X_test.values\n",
    "# y_train = y_train.values\n",
    "# y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCXn3Pzt6rL9"
   },
   "outputs": [],
   "source": [
    "X = X.values\n",
    "Y = Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAasI-VP5Bo8"
   },
   "outputs": [],
   "source": [
    "## NORMAL APPROACH ##\n",
    "\n",
    "\n",
    "# NB_EPOCHS = 1000  # num of epochs to test for\n",
    "# BATCH_SIZE = 16\n",
    "\n",
    "# ## Create our model\n",
    "# model = models.Sequential()\n",
    "\n",
    "# # 1st layer: input_dim=8, 12 nodes, RELU\n",
    "# model.add(layers.Dense(12, init='uniform', activation='relu',input_shape=(8,)))\n",
    "# # 2nd layer: 8 nodes, RELU\n",
    "# model.add(layers.Dense(8, init='uniform', activation='relu'))\n",
    "# # output layer: dim=1, activation sigmoid\n",
    "# model.add(layers.Dense(1, init='uniform', activation='sigmoid' ))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy',   # since we are predicting 0/1\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# # checkpoint: store the best model\n",
    "# ckpt_model = 'pima-weights.best.hdf5'\n",
    "# checkpoint = ModelCheckpoint(ckpt_model, \n",
    "#                             monitor='val_acc',\n",
    "#                             verbose=1,\n",
    "#                             save_best_only=True,\n",
    "#                             mode='max')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# print('Starting training...')\n",
    "# # train the model, store the results for plotting\n",
    "# history = model.fit(X_train,\n",
    "#                     y_train,\n",
    "#                     validation_data=(X_test, y_test),\n",
    "#                     nb_epoch=NB_EPOCHS,\n",
    "#                     batch_size=BATCH_SIZE,\n",
    "#                     callbacks=callbacks_list,\n",
    "#                     verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLO0T9h35EWD"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(12, activation='relu',input_shape=(8,)))\n",
    "  model.add(layers.Dense(8, activation='relu'))\n",
    "  model.add(layers.Dense(1, activation='sigmoid' ))\n",
    "  \n",
    "  model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 13753
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25901,
     "status": "ok",
     "timestamp": 1541939688791,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "GFnGgdpD3xj4",
    "outputId": "ea9f75d0-37f1-4b71-95ae-3a1ddff47c1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 1s 1ms/step - loss: 0.6641 - acc: 0.6545 - val_loss: 0.6654 - val_acc: 0.6406\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.6555 - acc: 0.6545 - val_loss: 0.6603 - val_acc: 0.6406\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6503 - acc: 0.6545 - val_loss: 0.6565 - val_acc: 0.6406\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.6463 - acc: 0.6545 - val_loss: 0.6523 - val_acc: 0.6406\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.6420 - acc: 0.6545 - val_loss: 0.6470 - val_acc: 0.6406\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.6377 - acc: 0.6545 - val_loss: 0.6426 - val_acc: 0.6406\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.6332 - acc: 0.6545 - val_loss: 0.6368 - val_acc: 0.6406\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.6280 - acc: 0.6545 - val_loss: 0.6306 - val_acc: 0.6406\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.6229 - acc: 0.6545 - val_loss: 0.6246 - val_acc: 0.6406\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 107us/step - loss: 0.6175 - acc: 0.6597 - val_loss: 0.6189 - val_acc: 0.6458\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.6131 - acc: 0.6753 - val_loss: 0.6134 - val_acc: 0.6510\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.6081 - acc: 0.6806 - val_loss: 0.6076 - val_acc: 0.6667\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 114us/step - loss: 0.6030 - acc: 0.6736 - val_loss: 0.6024 - val_acc: 0.6667\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.5977 - acc: 0.6875 - val_loss: 0.5965 - val_acc: 0.7083\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5925 - acc: 0.6962 - val_loss: 0.5917 - val_acc: 0.7083\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5876 - acc: 0.6927 - val_loss: 0.5867 - val_acc: 0.7083\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5820 - acc: 0.6910 - val_loss: 0.5816 - val_acc: 0.7240\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5767 - acc: 0.7031 - val_loss: 0.5758 - val_acc: 0.7344\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5710 - acc: 0.7135 - val_loss: 0.5721 - val_acc: 0.7396\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5654 - acc: 0.7135 - val_loss: 0.5677 - val_acc: 0.7396\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5596 - acc: 0.7222 - val_loss: 0.5632 - val_acc: 0.7344\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.5568 - acc: 0.7205 - val_loss: 0.5578 - val_acc: 0.7500\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 105us/step - loss: 0.5508 - acc: 0.7205 - val_loss: 0.5546 - val_acc: 0.7396\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5447 - acc: 0.7309 - val_loss: 0.5499 - val_acc: 0.7344\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5410 - acc: 0.7326 - val_loss: 0.5462 - val_acc: 0.7500\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5356 - acc: 0.7292 - val_loss: 0.5409 - val_acc: 0.7656\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5306 - acc: 0.7344 - val_loss: 0.5363 - val_acc: 0.7604\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5256 - acc: 0.7344 - val_loss: 0.5352 - val_acc: 0.7552\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5208 - acc: 0.7378 - val_loss: 0.5290 - val_acc: 0.7656\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5166 - acc: 0.7465 - val_loss: 0.5260 - val_acc: 0.7552\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5114 - acc: 0.7344 - val_loss: 0.5237 - val_acc: 0.7552\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5094 - acc: 0.7431 - val_loss: 0.5195 - val_acc: 0.7656\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5029 - acc: 0.7413 - val_loss: 0.5165 - val_acc: 0.7656\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4992 - acc: 0.7483 - val_loss: 0.5143 - val_acc: 0.7552\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4967 - acc: 0.7517 - val_loss: 0.5113 - val_acc: 0.7604\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4923 - acc: 0.7587 - val_loss: 0.5091 - val_acc: 0.7500\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4925 - acc: 0.7604 - val_loss: 0.5074 - val_acc: 0.7656\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4851 - acc: 0.7691 - val_loss: 0.5062 - val_acc: 0.7656\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4852 - acc: 0.7656 - val_loss: 0.5036 - val_acc: 0.7656\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4799 - acc: 0.7674 - val_loss: 0.5027 - val_acc: 0.7760\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4775 - acc: 0.7778 - val_loss: 0.5017 - val_acc: 0.7656\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4749 - acc: 0.7760 - val_loss: 0.5008 - val_acc: 0.7708\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 73us/step - loss: 0.4733 - acc: 0.7708 - val_loss: 0.5004 - val_acc: 0.7656\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4714 - acc: 0.7760 - val_loss: 0.4980 - val_acc: 0.7708\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4680 - acc: 0.7812 - val_loss: 0.4967 - val_acc: 0.7708\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4685 - acc: 0.7708 - val_loss: 0.4990 - val_acc: 0.7708\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4671 - acc: 0.7795 - val_loss: 0.4977 - val_acc: 0.7708\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4653 - acc: 0.7795 - val_loss: 0.4937 - val_acc: 0.7656\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4612 - acc: 0.7743 - val_loss: 0.4947 - val_acc: 0.7656\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4602 - acc: 0.7778 - val_loss: 0.4933 - val_acc: 0.7656\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4615 - acc: 0.7726 - val_loss: 0.4923 - val_acc: 0.7708\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4631 - acc: 0.7726 - val_loss: 0.4925 - val_acc: 0.7708\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4589 - acc: 0.7830 - val_loss: 0.4912 - val_acc: 0.7708\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4581 - acc: 0.7865 - val_loss: 0.4934 - val_acc: 0.7604\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4588 - acc: 0.7812 - val_loss: 0.4907 - val_acc: 0.7760\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4522 - acc: 0.7795 - val_loss: 0.4905 - val_acc: 0.7760\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4529 - acc: 0.7899 - val_loss: 0.4902 - val_acc: 0.7812\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4523 - acc: 0.7812 - val_loss: 0.4896 - val_acc: 0.7760\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 76us/step - loss: 0.4515 - acc: 0.7934 - val_loss: 0.4947 - val_acc: 0.7604\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4498 - acc: 0.7708 - val_loss: 0.4909 - val_acc: 0.7760\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4494 - acc: 0.7812 - val_loss: 0.4900 - val_acc: 0.7760\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4478 - acc: 0.7795 - val_loss: 0.4890 - val_acc: 0.7760\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4471 - acc: 0.7830 - val_loss: 0.4887 - val_acc: 0.7760\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4455 - acc: 0.7812 - val_loss: 0.4891 - val_acc: 0.7760\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4461 - acc: 0.7882 - val_loss: 0.4995 - val_acc: 0.7656\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4526 - acc: 0.7812 - val_loss: 0.4889 - val_acc: 0.7812\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4436 - acc: 0.7795 - val_loss: 0.4895 - val_acc: 0.7812\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4432 - acc: 0.7812 - val_loss: 0.4890 - val_acc: 0.7812\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4459 - acc: 0.7830 - val_loss: 0.4883 - val_acc: 0.7760\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4450 - acc: 0.7795 - val_loss: 0.4889 - val_acc: 0.7760\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4416 - acc: 0.7847 - val_loss: 0.4889 - val_acc: 0.7812\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4418 - acc: 0.7882 - val_loss: 0.4904 - val_acc: 0.7760\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4428 - acc: 0.7934 - val_loss: 0.4903 - val_acc: 0.7812\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4425 - acc: 0.7882 - val_loss: 0.4901 - val_acc: 0.7760\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4442 - acc: 0.8003 - val_loss: 0.4906 - val_acc: 0.7760\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4407 - acc: 0.7917 - val_loss: 0.4898 - val_acc: 0.7865\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4418 - acc: 0.7847 - val_loss: 0.4895 - val_acc: 0.7812\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4399 - acc: 0.7865 - val_loss: 0.4915 - val_acc: 0.7812\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4401 - acc: 0.7830 - val_loss: 0.4906 - val_acc: 0.7760\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4400 - acc: 0.7865 - val_loss: 0.4898 - val_acc: 0.7760\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4386 - acc: 0.7917 - val_loss: 0.4912 - val_acc: 0.7865\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4395 - acc: 0.7830 - val_loss: 0.4909 - val_acc: 0.7760\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4373 - acc: 0.7865 - val_loss: 0.4897 - val_acc: 0.7865\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4387 - acc: 0.7865 - val_loss: 0.4898 - val_acc: 0.7865\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4383 - acc: 0.7986 - val_loss: 0.4950 - val_acc: 0.7760\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4380 - acc: 0.7847 - val_loss: 0.4904 - val_acc: 0.7812\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4390 - acc: 0.7847 - val_loss: 0.4901 - val_acc: 0.7812\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4379 - acc: 0.7865 - val_loss: 0.4918 - val_acc: 0.7760\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4372 - acc: 0.7812 - val_loss: 0.4902 - val_acc: 0.7812\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4374 - acc: 0.7847 - val_loss: 0.4910 - val_acc: 0.7812\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4357 - acc: 0.7917 - val_loss: 0.4906 - val_acc: 0.7865\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4360 - acc: 0.7882 - val_loss: 0.4908 - val_acc: 0.7760\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4358 - acc: 0.7934 - val_loss: 0.4921 - val_acc: 0.7708\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4364 - acc: 0.7882 - val_loss: 0.4909 - val_acc: 0.7760\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4346 - acc: 0.7969 - val_loss: 0.4926 - val_acc: 0.7812\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4353 - acc: 0.7882 - val_loss: 0.4911 - val_acc: 0.7760\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4364 - acc: 0.7812 - val_loss: 0.4927 - val_acc: 0.7708\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4355 - acc: 0.7951 - val_loss: 0.4936 - val_acc: 0.7760\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4358 - acc: 0.7847 - val_loss: 0.4916 - val_acc: 0.7812\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4348 - acc: 0.7830 - val_loss: 0.4947 - val_acc: 0.7760\n",
      "processing fold # 1\n",
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 527us/step - loss: 0.6644 - acc: 0.6667 - val_loss: 0.6733 - val_acc: 0.6042\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.6496 - acc: 0.6667 - val_loss: 0.6726 - val_acc: 0.6042\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.6430 - acc: 0.6667 - val_loss: 0.6714 - val_acc: 0.6042\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.6370 - acc: 0.6667 - val_loss: 0.6653 - val_acc: 0.6042\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.6319 - acc: 0.6667 - val_loss: 0.6625 - val_acc: 0.6042\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.6259 - acc: 0.6667 - val_loss: 0.6559 - val_acc: 0.6042\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.6197 - acc: 0.6667 - val_loss: 0.6486 - val_acc: 0.6042\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.6108 - acc: 0.6667 - val_loss: 0.6438 - val_acc: 0.6042\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.6022 - acc: 0.6684 - val_loss: 0.6361 - val_acc: 0.6146\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.5934 - acc: 0.6736 - val_loss: 0.6323 - val_acc: 0.6250\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5854 - acc: 0.6910 - val_loss: 0.6254 - val_acc: 0.6406\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5764 - acc: 0.7083 - val_loss: 0.6188 - val_acc: 0.6615\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5679 - acc: 0.7066 - val_loss: 0.6127 - val_acc: 0.6406\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5603 - acc: 0.7361 - val_loss: 0.6063 - val_acc: 0.6667\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5526 - acc: 0.7396 - val_loss: 0.6045 - val_acc: 0.6615\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5449 - acc: 0.7344 - val_loss: 0.5952 - val_acc: 0.6823\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5370 - acc: 0.7413 - val_loss: 0.5949 - val_acc: 0.6875\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.5319 - acc: 0.7431 - val_loss: 0.5863 - val_acc: 0.6562\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5302 - acc: 0.7292 - val_loss: 0.5878 - val_acc: 0.6927\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5211 - acc: 0.7465 - val_loss: 0.5836 - val_acc: 0.7031\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5164 - acc: 0.7552 - val_loss: 0.5824 - val_acc: 0.6771\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5111 - acc: 0.7535 - val_loss: 0.5768 - val_acc: 0.6719\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5072 - acc: 0.7535 - val_loss: 0.5745 - val_acc: 0.6719\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5051 - acc: 0.7517 - val_loss: 0.5749 - val_acc: 0.6823\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5037 - acc: 0.7604 - val_loss: 0.5706 - val_acc: 0.6823\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4971 - acc: 0.7656 - val_loss: 0.5718 - val_acc: 0.6979\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4948 - acc: 0.7708 - val_loss: 0.5662 - val_acc: 0.6927\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.4904 - acc: 0.7691 - val_loss: 0.5654 - val_acc: 0.6979\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4876 - acc: 0.7691 - val_loss: 0.5622 - val_acc: 0.7031\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4858 - acc: 0.7726 - val_loss: 0.5583 - val_acc: 0.6979\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4858 - acc: 0.7656 - val_loss: 0.5564 - val_acc: 0.6927\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4790 - acc: 0.7778 - val_loss: 0.5649 - val_acc: 0.6875\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4791 - acc: 0.7726 - val_loss: 0.5527 - val_acc: 0.6979\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4762 - acc: 0.7830 - val_loss: 0.5572 - val_acc: 0.7083\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4736 - acc: 0.7830 - val_loss: 0.5496 - val_acc: 0.7031\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4718 - acc: 0.7830 - val_loss: 0.5497 - val_acc: 0.7083\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4705 - acc: 0.7847 - val_loss: 0.5487 - val_acc: 0.7083\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4698 - acc: 0.7812 - val_loss: 0.5485 - val_acc: 0.7083\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4663 - acc: 0.7865 - val_loss: 0.5511 - val_acc: 0.6979\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4644 - acc: 0.7899 - val_loss: 0.5424 - val_acc: 0.7031\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4634 - acc: 0.7899 - val_loss: 0.5428 - val_acc: 0.7135\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4606 - acc: 0.7917 - val_loss: 0.5451 - val_acc: 0.7083\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4586 - acc: 0.7951 - val_loss: 0.5380 - val_acc: 0.7083\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4574 - acc: 0.7934 - val_loss: 0.5396 - val_acc: 0.7135\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4562 - acc: 0.7934 - val_loss: 0.5377 - val_acc: 0.7135\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4565 - acc: 0.7882 - val_loss: 0.5373 - val_acc: 0.7135\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4544 - acc: 0.7951 - val_loss: 0.5420 - val_acc: 0.7188\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4547 - acc: 0.8021 - val_loss: 0.5331 - val_acc: 0.6979\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4567 - acc: 0.8021 - val_loss: 0.5302 - val_acc: 0.7083\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4523 - acc: 0.7865 - val_loss: 0.5330 - val_acc: 0.7135\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4495 - acc: 0.7917 - val_loss: 0.5294 - val_acc: 0.7083\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4471 - acc: 0.7934 - val_loss: 0.5298 - val_acc: 0.7188\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4459 - acc: 0.7969 - val_loss: 0.5274 - val_acc: 0.7083\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4468 - acc: 0.8003 - val_loss: 0.5372 - val_acc: 0.7240\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4469 - acc: 0.8003 - val_loss: 0.5288 - val_acc: 0.7135\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4437 - acc: 0.7986 - val_loss: 0.5262 - val_acc: 0.7083\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4426 - acc: 0.8003 - val_loss: 0.5287 - val_acc: 0.7188\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4414 - acc: 0.8038 - val_loss: 0.5269 - val_acc: 0.7135\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4403 - acc: 0.7986 - val_loss: 0.5228 - val_acc: 0.7083\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4450 - acc: 0.7934 - val_loss: 0.5305 - val_acc: 0.7344\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4389 - acc: 0.8056 - val_loss: 0.5224 - val_acc: 0.7188\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4384 - acc: 0.8038 - val_loss: 0.5226 - val_acc: 0.7083\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4382 - acc: 0.8021 - val_loss: 0.5207 - val_acc: 0.7135\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4371 - acc: 0.8003 - val_loss: 0.5309 - val_acc: 0.7396\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4384 - acc: 0.8038 - val_loss: 0.5224 - val_acc: 0.7083\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4370 - acc: 0.8021 - val_loss: 0.5168 - val_acc: 0.7188\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4343 - acc: 0.7934 - val_loss: 0.5252 - val_acc: 0.7240\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4345 - acc: 0.8056 - val_loss: 0.5201 - val_acc: 0.7083\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4329 - acc: 0.8090 - val_loss: 0.5178 - val_acc: 0.7188\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4330 - acc: 0.8021 - val_loss: 0.5165 - val_acc: 0.7135\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4340 - acc: 0.8073 - val_loss: 0.5176 - val_acc: 0.7135\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4323 - acc: 0.8003 - val_loss: 0.5229 - val_acc: 0.7188\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4311 - acc: 0.8038 - val_loss: 0.5137 - val_acc: 0.7135\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4293 - acc: 0.8073 - val_loss: 0.5154 - val_acc: 0.7083\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4286 - acc: 0.8056 - val_loss: 0.5163 - val_acc: 0.7083\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4287 - acc: 0.8090 - val_loss: 0.5169 - val_acc: 0.7188\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4320 - acc: 0.7969 - val_loss: 0.5194 - val_acc: 0.7240\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4277 - acc: 0.8003 - val_loss: 0.5142 - val_acc: 0.7135\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4286 - acc: 0.8038 - val_loss: 0.5134 - val_acc: 0.7135\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4295 - acc: 0.8003 - val_loss: 0.5124 - val_acc: 0.7292\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4287 - acc: 0.8038 - val_loss: 0.5119 - val_acc: 0.7135\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4275 - acc: 0.8073 - val_loss: 0.5117 - val_acc: 0.7135\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4250 - acc: 0.8073 - val_loss: 0.5113 - val_acc: 0.7135\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4279 - acc: 0.8056 - val_loss: 0.5175 - val_acc: 0.7292\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4249 - acc: 0.8038 - val_loss: 0.5101 - val_acc: 0.7135\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4240 - acc: 0.8108 - val_loss: 0.5160 - val_acc: 0.7240\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 122us/step - loss: 0.4245 - acc: 0.8021 - val_loss: 0.5172 - val_acc: 0.7240\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4251 - acc: 0.8038 - val_loss: 0.5104 - val_acc: 0.7135\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4228 - acc: 0.8003 - val_loss: 0.5100 - val_acc: 0.7135\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4223 - acc: 0.8038 - val_loss: 0.5101 - val_acc: 0.7188\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4232 - acc: 0.8003 - val_loss: 0.5170 - val_acc: 0.7188\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4228 - acc: 0.8021 - val_loss: 0.5091 - val_acc: 0.7135\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4212 - acc: 0.8056 - val_loss: 0.5094 - val_acc: 0.7135\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4207 - acc: 0.8056 - val_loss: 0.5143 - val_acc: 0.7240\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4215 - acc: 0.8056 - val_loss: 0.5104 - val_acc: 0.7344\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4216 - acc: 0.8090 - val_loss: 0.5069 - val_acc: 0.7083\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4226 - acc: 0.8142 - val_loss: 0.5150 - val_acc: 0.7240\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4204 - acc: 0.8090 - val_loss: 0.5090 - val_acc: 0.7188\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4194 - acc: 0.8038 - val_loss: 0.5095 - val_acc: 0.7135\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4199 - acc: 0.8090 - val_loss: 0.5124 - val_acc: 0.7344\n",
      "processing fold # 2\n",
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 707us/step - loss: 0.7112 - acc: 0.3802 - val_loss: 0.7035 - val_acc: 0.3073\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6881 - acc: 0.5885 - val_loss: 0.6717 - val_acc: 0.7500\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.6754 - acc: 0.6667 - val_loss: 0.6552 - val_acc: 0.7292\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.6683 - acc: 0.6285 - val_loss: 0.6418 - val_acc: 0.7292\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.6625 - acc: 0.6372 - val_loss: 0.6339 - val_acc: 0.7292\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.6565 - acc: 0.6406 - val_loss: 0.6266 - val_acc: 0.7292\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.6509 - acc: 0.6493 - val_loss: 0.6156 - val_acc: 0.7292\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.6447 - acc: 0.6476 - val_loss: 0.6092 - val_acc: 0.7344\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.6379 - acc: 0.6528 - val_loss: 0.6042 - val_acc: 0.7604\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.6308 - acc: 0.6562 - val_loss: 0.5934 - val_acc: 0.7604\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.6225 - acc: 0.6701 - val_loss: 0.5914 - val_acc: 0.7448\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.6143 - acc: 0.6927 - val_loss: 0.5816 - val_acc: 0.7448\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 122us/step - loss: 0.6065 - acc: 0.6979 - val_loss: 0.5764 - val_acc: 0.7448\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5988 - acc: 0.6962 - val_loss: 0.5678 - val_acc: 0.7500\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5914 - acc: 0.7153 - val_loss: 0.5602 - val_acc: 0.7500\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.5830 - acc: 0.7188 - val_loss: 0.5568 - val_acc: 0.7396\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5784 - acc: 0.7101 - val_loss: 0.5473 - val_acc: 0.7396\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5707 - acc: 0.7188 - val_loss: 0.5405 - val_acc: 0.7396\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.5661 - acc: 0.7135 - val_loss: 0.5388 - val_acc: 0.7500\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5623 - acc: 0.7240 - val_loss: 0.5339 - val_acc: 0.7552\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5566 - acc: 0.7153 - val_loss: 0.5284 - val_acc: 0.7656\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5503 - acc: 0.7292 - val_loss: 0.5256 - val_acc: 0.7604\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5470 - acc: 0.7188 - val_loss: 0.5189 - val_acc: 0.7656\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5435 - acc: 0.7431 - val_loss: 0.5097 - val_acc: 0.7708\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 116us/step - loss: 0.5440 - acc: 0.7274 - val_loss: 0.5040 - val_acc: 0.7604\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5365 - acc: 0.7205 - val_loss: 0.5160 - val_acc: 0.7708\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.5339 - acc: 0.7361 - val_loss: 0.5004 - val_acc: 0.7812\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5319 - acc: 0.7378 - val_loss: 0.5260 - val_acc: 0.7396\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5295 - acc: 0.7396 - val_loss: 0.4991 - val_acc: 0.7812\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.5274 - acc: 0.7378 - val_loss: 0.5024 - val_acc: 0.7760\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 100us/step - loss: 0.5253 - acc: 0.7413 - val_loss: 0.5038 - val_acc: 0.7604\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5209 - acc: 0.7431 - val_loss: 0.4931 - val_acc: 0.7812\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.5194 - acc: 0.7535 - val_loss: 0.5010 - val_acc: 0.7760\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.5177 - acc: 0.7500 - val_loss: 0.4909 - val_acc: 0.7760\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5155 - acc: 0.7500 - val_loss: 0.4899 - val_acc: 0.7708\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5122 - acc: 0.7535 - val_loss: 0.4893 - val_acc: 0.7760\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5102 - acc: 0.7552 - val_loss: 0.4810 - val_acc: 0.7760\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.5084 - acc: 0.7552 - val_loss: 0.4845 - val_acc: 0.7812\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.5076 - acc: 0.7656 - val_loss: 0.4865 - val_acc: 0.7865\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5048 - acc: 0.7639 - val_loss: 0.4716 - val_acc: 0.7917\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5062 - acc: 0.7622 - val_loss: 0.4811 - val_acc: 0.7917\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.5024 - acc: 0.7656 - val_loss: 0.4752 - val_acc: 0.7865\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5007 - acc: 0.7639 - val_loss: 0.4790 - val_acc: 0.7917\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4993 - acc: 0.7656 - val_loss: 0.4697 - val_acc: 0.7969\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4982 - acc: 0.7587 - val_loss: 0.4797 - val_acc: 0.7812\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4987 - acc: 0.7535 - val_loss: 0.4695 - val_acc: 0.7969\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4979 - acc: 0.7639 - val_loss: 0.4642 - val_acc: 0.7969\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4946 - acc: 0.7622 - val_loss: 0.4623 - val_acc: 0.8021\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4946 - acc: 0.7691 - val_loss: 0.4719 - val_acc: 0.7917\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4939 - acc: 0.7674 - val_loss: 0.4697 - val_acc: 0.8021\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4926 - acc: 0.7691 - val_loss: 0.4746 - val_acc: 0.7708\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4904 - acc: 0.7604 - val_loss: 0.4637 - val_acc: 0.7969\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4901 - acc: 0.7674 - val_loss: 0.4637 - val_acc: 0.7969\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4890 - acc: 0.7604 - val_loss: 0.4636 - val_acc: 0.7969\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4884 - acc: 0.7674 - val_loss: 0.4581 - val_acc: 0.8073\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 105us/step - loss: 0.4877 - acc: 0.7674 - val_loss: 0.4630 - val_acc: 0.7917\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4874 - acc: 0.7587 - val_loss: 0.4588 - val_acc: 0.8021\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4881 - acc: 0.7587 - val_loss: 0.4704 - val_acc: 0.7708\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4866 - acc: 0.7674 - val_loss: 0.4662 - val_acc: 0.7812\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4866 - acc: 0.7656 - val_loss: 0.4522 - val_acc: 0.8021\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4861 - acc: 0.7604 - val_loss: 0.4587 - val_acc: 0.7865\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 102us/step - loss: 0.4859 - acc: 0.7639 - val_loss: 0.4673 - val_acc: 0.7760\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.4830 - acc: 0.7604 - val_loss: 0.4558 - val_acc: 0.7969\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4823 - acc: 0.7674 - val_loss: 0.4541 - val_acc: 0.7969\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4818 - acc: 0.7587 - val_loss: 0.4535 - val_acc: 0.7969\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4807 - acc: 0.7604 - val_loss: 0.4544 - val_acc: 0.7917\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4803 - acc: 0.7674 - val_loss: 0.4613 - val_acc: 0.7812\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4815 - acc: 0.7587 - val_loss: 0.4583 - val_acc: 0.7812\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4796 - acc: 0.7674 - val_loss: 0.4605 - val_acc: 0.7760\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4803 - acc: 0.7587 - val_loss: 0.4612 - val_acc: 0.7865\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4790 - acc: 0.7656 - val_loss: 0.4517 - val_acc: 0.7865\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4789 - acc: 0.7604 - val_loss: 0.4610 - val_acc: 0.7812\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4779 - acc: 0.7569 - val_loss: 0.4578 - val_acc: 0.7760\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.4771 - acc: 0.7622 - val_loss: 0.4542 - val_acc: 0.7812\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4751 - acc: 0.7604 - val_loss: 0.4494 - val_acc: 0.7812\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4760 - acc: 0.7639 - val_loss: 0.4494 - val_acc: 0.7865\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4754 - acc: 0.7604 - val_loss: 0.4496 - val_acc: 0.7865\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4741 - acc: 0.7604 - val_loss: 0.4547 - val_acc: 0.7760\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4736 - acc: 0.7639 - val_loss: 0.4476 - val_acc: 0.7865\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4756 - acc: 0.7622 - val_loss: 0.4484 - val_acc: 0.7865\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4823 - acc: 0.7708 - val_loss: 0.4678 - val_acc: 0.7865\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 99us/step - loss: 0.4721 - acc: 0.7622 - val_loss: 0.4474 - val_acc: 0.7969\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4741 - acc: 0.7604 - val_loss: 0.4473 - val_acc: 0.7917\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4727 - acc: 0.7639 - val_loss: 0.4472 - val_acc: 0.7865\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4714 - acc: 0.7622 - val_loss: 0.4477 - val_acc: 0.7865\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.4730 - acc: 0.7708 - val_loss: 0.4478 - val_acc: 0.7865\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 94us/step - loss: 0.4717 - acc: 0.7604 - val_loss: 0.4506 - val_acc: 0.7760\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 93us/step - loss: 0.4712 - acc: 0.7656 - val_loss: 0.4498 - val_acc: 0.7760\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.4709 - acc: 0.7691 - val_loss: 0.4528 - val_acc: 0.7812\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4692 - acc: 0.7587 - val_loss: 0.4478 - val_acc: 0.7812\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4694 - acc: 0.7674 - val_loss: 0.4557 - val_acc: 0.7760\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4701 - acc: 0.7639 - val_loss: 0.4533 - val_acc: 0.7760\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4696 - acc: 0.7656 - val_loss: 0.4529 - val_acc: 0.7760\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4681 - acc: 0.7639 - val_loss: 0.4485 - val_acc: 0.7760\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4690 - acc: 0.7726 - val_loss: 0.4501 - val_acc: 0.7760\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4684 - acc: 0.7674 - val_loss: 0.4561 - val_acc: 0.7760\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4694 - acc: 0.7622 - val_loss: 0.4432 - val_acc: 0.7969\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4707 - acc: 0.7708 - val_loss: 0.4477 - val_acc: 0.7760\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4721 - acc: 0.7587 - val_loss: 0.4509 - val_acc: 0.7865\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4664 - acc: 0.7726 - val_loss: 0.4476 - val_acc: 0.7812\n",
      "processing fold # 3\n",
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/100\n",
      "576/576 [==============================] - 0s 775us/step - loss: 0.6942 - acc: 0.4809 - val_loss: 0.6741 - val_acc: 0.6510\n",
      "Epoch 2/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.6589 - acc: 0.6771 - val_loss: 0.6610 - val_acc: 0.6354\n",
      "Epoch 3/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.6447 - acc: 0.6684 - val_loss: 0.6526 - val_acc: 0.6354\n",
      "Epoch 4/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.6343 - acc: 0.6753 - val_loss: 0.6427 - val_acc: 0.6302\n",
      "Epoch 5/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6249 - acc: 0.6840 - val_loss: 0.6357 - val_acc: 0.6302\n",
      "Epoch 6/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.6176 - acc: 0.7031 - val_loss: 0.6294 - val_acc: 0.6302\n",
      "Epoch 7/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.6091 - acc: 0.6962 - val_loss: 0.6183 - val_acc: 0.6458\n",
      "Epoch 8/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.6006 - acc: 0.7083 - val_loss: 0.6096 - val_acc: 0.6667\n",
      "Epoch 9/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5924 - acc: 0.6997 - val_loss: 0.6014 - val_acc: 0.6562\n",
      "Epoch 10/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5862 - acc: 0.7083 - val_loss: 0.5916 - val_acc: 0.6979\n",
      "Epoch 11/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5809 - acc: 0.7118 - val_loss: 0.5858 - val_acc: 0.6823\n",
      "Epoch 12/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5725 - acc: 0.7240 - val_loss: 0.5754 - val_acc: 0.7083\n",
      "Epoch 13/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5634 - acc: 0.7240 - val_loss: 0.5657 - val_acc: 0.7188\n",
      "Epoch 14/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5559 - acc: 0.7240 - val_loss: 0.5602 - val_acc: 0.7240\n",
      "Epoch 15/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5514 - acc: 0.7309 - val_loss: 0.5481 - val_acc: 0.7604\n",
      "Epoch 16/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.5455 - acc: 0.7326 - val_loss: 0.5428 - val_acc: 0.7396\n",
      "Epoch 17/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.5404 - acc: 0.7396 - val_loss: 0.5347 - val_acc: 0.7812\n",
      "Epoch 18/100\n",
      "576/576 [==============================] - 0s 98us/step - loss: 0.5356 - acc: 0.7535 - val_loss: 0.5335 - val_acc: 0.7396\n",
      "Epoch 19/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5319 - acc: 0.7431 - val_loss: 0.5233 - val_acc: 0.7708\n",
      "Epoch 20/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5292 - acc: 0.7448 - val_loss: 0.5175 - val_acc: 0.7760\n",
      "Epoch 21/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5247 - acc: 0.7517 - val_loss: 0.5152 - val_acc: 0.7500\n",
      "Epoch 22/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.5219 - acc: 0.7604 - val_loss: 0.5091 - val_acc: 0.7812\n",
      "Epoch 23/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.5191 - acc: 0.7465 - val_loss: 0.5033 - val_acc: 0.7760\n",
      "Epoch 24/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.5156 - acc: 0.7535 - val_loss: 0.5054 - val_acc: 0.7552\n",
      "Epoch 25/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5113 - acc: 0.7587 - val_loss: 0.4984 - val_acc: 0.7812\n",
      "Epoch 26/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5118 - acc: 0.7622 - val_loss: 0.5065 - val_acc: 0.7396\n",
      "Epoch 27/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5083 - acc: 0.7656 - val_loss: 0.4909 - val_acc: 0.7917\n",
      "Epoch 28/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5064 - acc: 0.7639 - val_loss: 0.4879 - val_acc: 0.7917\n",
      "Epoch 29/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5055 - acc: 0.7639 - val_loss: 0.4852 - val_acc: 0.7917\n",
      "Epoch 30/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5032 - acc: 0.7604 - val_loss: 0.4840 - val_acc: 0.8021\n",
      "Epoch 31/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4985 - acc: 0.7604 - val_loss: 0.4856 - val_acc: 0.7760\n",
      "Epoch 32/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5012 - acc: 0.7569 - val_loss: 0.4797 - val_acc: 0.7812\n",
      "Epoch 33/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4952 - acc: 0.7691 - val_loss: 0.4752 - val_acc: 0.7969\n",
      "Epoch 34/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4974 - acc: 0.7743 - val_loss: 0.4728 - val_acc: 0.7812\n",
      "Epoch 35/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4916 - acc: 0.7552 - val_loss: 0.4652 - val_acc: 0.7969\n",
      "Epoch 36/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4886 - acc: 0.7622 - val_loss: 0.4685 - val_acc: 0.7812\n",
      "Epoch 37/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4866 - acc: 0.7708 - val_loss: 0.4605 - val_acc: 0.8021\n",
      "Epoch 38/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4833 - acc: 0.7691 - val_loss: 0.4642 - val_acc: 0.7917\n",
      "Epoch 39/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4826 - acc: 0.7726 - val_loss: 0.4581 - val_acc: 0.7969\n",
      "Epoch 40/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4799 - acc: 0.7760 - val_loss: 0.4546 - val_acc: 0.7969\n",
      "Epoch 41/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4797 - acc: 0.7674 - val_loss: 0.4572 - val_acc: 0.7969\n",
      "Epoch 42/100\n",
      "576/576 [==============================] - 0s 108us/step - loss: 0.4786 - acc: 0.7778 - val_loss: 0.4553 - val_acc: 0.7917\n",
      "Epoch 43/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4796 - acc: 0.7708 - val_loss: 0.4527 - val_acc: 0.8021\n",
      "Epoch 44/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4762 - acc: 0.7708 - val_loss: 0.4529 - val_acc: 0.7865\n",
      "Epoch 45/100\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4741 - acc: 0.7743 - val_loss: 0.4517 - val_acc: 0.7917\n",
      "Epoch 46/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4734 - acc: 0.7778 - val_loss: 0.4488 - val_acc: 0.8021\n",
      "Epoch 47/100\n",
      "576/576 [==============================] - 0s 95us/step - loss: 0.4766 - acc: 0.7743 - val_loss: 0.4529 - val_acc: 0.8177\n",
      "Epoch 48/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4714 - acc: 0.7778 - val_loss: 0.4523 - val_acc: 0.7865\n",
      "Epoch 49/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4689 - acc: 0.7726 - val_loss: 0.4457 - val_acc: 0.7812\n",
      "Epoch 50/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4688 - acc: 0.7847 - val_loss: 0.4474 - val_acc: 0.7812\n",
      "Epoch 51/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4675 - acc: 0.7795 - val_loss: 0.4459 - val_acc: 0.7760\n",
      "Epoch 52/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4673 - acc: 0.7812 - val_loss: 0.4456 - val_acc: 0.7969\n",
      "Epoch 53/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4638 - acc: 0.7812 - val_loss: 0.4483 - val_acc: 0.7865\n",
      "Epoch 54/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4654 - acc: 0.7847 - val_loss: 0.4467 - val_acc: 0.7760\n",
      "Epoch 55/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4641 - acc: 0.7812 - val_loss: 0.4458 - val_acc: 0.7812\n",
      "Epoch 56/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4681 - acc: 0.7795 - val_loss: 0.4447 - val_acc: 0.7812\n",
      "Epoch 57/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4637 - acc: 0.7795 - val_loss: 0.4439 - val_acc: 0.7812\n",
      "Epoch 58/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4688 - acc: 0.7778 - val_loss: 0.4455 - val_acc: 0.8177\n",
      "Epoch 59/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4651 - acc: 0.7778 - val_loss: 0.4438 - val_acc: 0.8125\n",
      "Epoch 60/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4618 - acc: 0.7812 - val_loss: 0.4444 - val_acc: 0.7760\n",
      "Epoch 61/100\n",
      "576/576 [==============================] - 0s 116us/step - loss: 0.4622 - acc: 0.7882 - val_loss: 0.4436 - val_acc: 0.8021\n",
      "Epoch 62/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4600 - acc: 0.7830 - val_loss: 0.4450 - val_acc: 0.7812\n",
      "Epoch 63/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4583 - acc: 0.7778 - val_loss: 0.4457 - val_acc: 0.8125\n",
      "Epoch 64/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4609 - acc: 0.7847 - val_loss: 0.4422 - val_acc: 0.7865\n",
      "Epoch 65/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4624 - acc: 0.7812 - val_loss: 0.4414 - val_acc: 0.8073\n",
      "Epoch 66/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4613 - acc: 0.7743 - val_loss: 0.4428 - val_acc: 0.7865\n",
      "Epoch 67/100\n",
      "576/576 [==============================] - 0s 101us/step - loss: 0.4666 - acc: 0.7812 - val_loss: 0.4424 - val_acc: 0.7812\n",
      "Epoch 68/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4569 - acc: 0.7847 - val_loss: 0.4416 - val_acc: 0.7917\n",
      "Epoch 69/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4593 - acc: 0.7778 - val_loss: 0.4436 - val_acc: 0.7865\n",
      "Epoch 70/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4564 - acc: 0.7865 - val_loss: 0.4481 - val_acc: 0.7812\n",
      "Epoch 71/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4575 - acc: 0.7778 - val_loss: 0.4418 - val_acc: 0.8073\n",
      "Epoch 72/100\n",
      "576/576 [==============================] - 0s 91us/step - loss: 0.4564 - acc: 0.7795 - val_loss: 0.4403 - val_acc: 0.7969\n",
      "Epoch 73/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4559 - acc: 0.7778 - val_loss: 0.4424 - val_acc: 0.7865\n",
      "Epoch 74/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4571 - acc: 0.7812 - val_loss: 0.4383 - val_acc: 0.7865\n",
      "Epoch 75/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4580 - acc: 0.7760 - val_loss: 0.4422 - val_acc: 0.8073\n",
      "Epoch 76/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4554 - acc: 0.7899 - val_loss: 0.4408 - val_acc: 0.7917\n",
      "Epoch 77/100\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4562 - acc: 0.7812 - val_loss: 0.4457 - val_acc: 0.7812\n",
      "Epoch 78/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4556 - acc: 0.7882 - val_loss: 0.4423 - val_acc: 0.7865\n",
      "Epoch 79/100\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.4602 - acc: 0.7726 - val_loss: 0.4417 - val_acc: 0.8073\n",
      "Epoch 80/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4544 - acc: 0.7865 - val_loss: 0.4379 - val_acc: 0.8125\n",
      "Epoch 81/100\n",
      "576/576 [==============================] - 0s 83us/step - loss: 0.4547 - acc: 0.7778 - val_loss: 0.4398 - val_acc: 0.8021\n",
      "Epoch 82/100\n",
      "576/576 [==============================] - 0s 88us/step - loss: 0.4554 - acc: 0.7882 - val_loss: 0.4420 - val_acc: 0.7917\n",
      "Epoch 83/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4522 - acc: 0.7917 - val_loss: 0.4395 - val_acc: 0.8021\n",
      "Epoch 84/100\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.4564 - acc: 0.7847 - val_loss: 0.4603 - val_acc: 0.7708\n",
      "Epoch 85/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4567 - acc: 0.7812 - val_loss: 0.4395 - val_acc: 0.8073\n",
      "Epoch 86/100\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4527 - acc: 0.7812 - val_loss: 0.4401 - val_acc: 0.7917\n",
      "Epoch 87/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4555 - acc: 0.7812 - val_loss: 0.4457 - val_acc: 0.7865\n",
      "Epoch 88/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4533 - acc: 0.7847 - val_loss: 0.4425 - val_acc: 0.7917\n",
      "Epoch 89/100\n",
      "576/576 [==============================] - 0s 96us/step - loss: 0.4560 - acc: 0.7795 - val_loss: 0.4416 - val_acc: 0.8073\n",
      "Epoch 90/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4526 - acc: 0.7795 - val_loss: 0.4404 - val_acc: 0.8021\n",
      "Epoch 91/100\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4540 - acc: 0.7865 - val_loss: 0.4412 - val_acc: 0.8021\n",
      "Epoch 92/100\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.4555 - acc: 0.7917 - val_loss: 0.4404 - val_acc: 0.7969\n",
      "Epoch 93/100\n",
      "576/576 [==============================] - 0s 89us/step - loss: 0.4531 - acc: 0.7812 - val_loss: 0.4396 - val_acc: 0.8021\n",
      "Epoch 94/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4529 - acc: 0.7812 - val_loss: 0.4526 - val_acc: 0.7865\n",
      "Epoch 95/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4525 - acc: 0.7778 - val_loss: 0.4401 - val_acc: 0.8021\n",
      "Epoch 96/100\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4541 - acc: 0.7847 - val_loss: 0.4410 - val_acc: 0.7969\n",
      "Epoch 97/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4512 - acc: 0.7882 - val_loss: 0.4380 - val_acc: 0.8125\n",
      "Epoch 98/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4511 - acc: 0.7830 - val_loss: 0.4398 - val_acc: 0.8073\n",
      "Epoch 99/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4544 - acc: 0.7760 - val_loss: 0.4459 - val_acc: 0.7969\n",
      "Epoch 100/100\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4497 - acc: 0.7812 - val_loss: 0.4438 - val_acc: 0.7969\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "num_val_samples = len(X) // k\n",
    "num_epochs = 100\n",
    "#all_scores = []\n",
    "all_val_acc_history=[]\n",
    "#all_loss_history=[]\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  partial_train_data = np.concatenate([X[:i * num_val_samples],X[(i + 1) * num_val_samples:]],axis=0)\n",
    "  partial_train_targets = np.concatenate([Y[:i * num_val_samples],Y[(i + 1) * num_val_samples:]],axis=0)\n",
    "  model = build_model()\n",
    "  x = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets),epochs=num_epochs, batch_size=16)\n",
    "  val_acc_history = x.history['val_acc']\n",
    "  all_val_acc_history.append(val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orqaw2_gXHFN"
   },
   "outputs": [],
   "source": [
    "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_history]) for i in range(num_epochs)]\n",
    "#average_loss_history = [np.mean([x[i] for x in all_loss_history]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sfsQmvRaltb"
   },
   "outputs": [],
   "source": [
    "#min(average_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 876,
     "status": "ok",
     "timestamp": 1541939781114,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "tLtA0taApKs4",
    "outputId": "ec14a1f5-e47b-4e8f-f2d7-e1f77462678f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7760416666666667"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(average_val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqava_kMqmyh"
   },
   "outputs": [],
   "source": [
    "#min(x.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 973
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2498,
     "status": "ok",
     "timestamp": 1541939732618,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "RY9OG2CPXZMd",
    "outputId": "8279a47a-96d5-455d-f7b2-5d8dbb578a77"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA98AAAO8CAYAAABQkwdEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a91b190b8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "  smoothed_points = []\n",
    "  for point in points:\n",
    "    if smoothed_points:\n",
    "      previous = smoothed_points[-1]\n",
    "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "    else:\n",
    "      smoothed_points.append(point)\n",
    "  return smoothed_points\n",
    "\n",
    "smooth_val_acc_history = smooth_curve(average_val_acc_history[10:])\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(num=None, figsize=(15, 15), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(range(1, len(smooth_val_acc_history) + 1), smooth_val_acc_history)\n",
    "plt.xticks(np.arange(min(range(1, len(smooth_val_acc_history) + 1)), max(range(1, len(smooth_val_acc_history) + 1))+1, 2.0))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYFYODSyZzlH"
   },
   "outputs": [],
   "source": [
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxvqAkAwZ_Po"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H04IVPC0OqG4"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1759,
     "status": "ok",
     "timestamp": 1541940331360,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "sl_pDJ7EPOT2",
    "outputId": "7b74576e-ce15-4ee1-f2b1-4f047aff2fe7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1019,
     "status": "ok",
     "timestamp": 1541940332485,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "XIo7TmjoPQvG",
    "outputId": "78ca2aa6-dad8-421b-af95-ddc6334fbf8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1002,
     "status": "ok",
     "timestamp": 1541940333638,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "AS2lQM_IRbVz",
    "outputId": "59650a1c-7f3f-47e8-dd49-754b1b03527f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1026,
     "status": "ok",
     "timestamp": 1541940334775,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "Vi3sbVEBRdwL",
    "outputId": "4633708e-4219-42c8-d8a0-f3d2623f816b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJAgRiQlx3n0"
   },
   "outputs": [],
   "source": [
    "model2 = models.Sequential()\n",
    "model2.add(layers.Dense(12, activation='relu',input_shape=(8,)))\n",
    "model2.add(layers.Dense(8, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid' ))\n",
    "  \n",
    "#   model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "#   return model\n",
    "# x = model.fit(X_train, y_train, epochs=81, batch_size=16, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LBfnY8CDTVu8"
   },
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1887
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4928,
     "status": "ok",
     "timestamp": 1541940342966,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "nD0PRgDhThjN",
    "outputId": "d0e7182c-ee5b-42e8-cf73-60fddd6e562e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/55\n",
      "576/576 [==============================] - 1s 2ms/step - loss: 0.6677 - acc: 0.6424\n",
      "Epoch 2/55\n",
      "576/576 [==============================] - 0s 129us/step - loss: 0.6617 - acc: 0.6424\n",
      "Epoch 3/55\n",
      "576/576 [==============================] - 0s 109us/step - loss: 0.6573 - acc: 0.6424\n",
      "Epoch 4/55\n",
      "576/576 [==============================] - 0s 112us/step - loss: 0.6538 - acc: 0.6424\n",
      "Epoch 5/55\n",
      "576/576 [==============================] - 0s 131us/step - loss: 0.6507 - acc: 0.6424\n",
      "Epoch 6/55\n",
      "576/576 [==============================] - 0s 110us/step - loss: 0.6462 - acc: 0.6424\n",
      "Epoch 7/55\n",
      "576/576 [==============================] - 0s 130us/step - loss: 0.6421 - acc: 0.6424\n",
      "Epoch 8/55\n",
      "576/576 [==============================] - 0s 111us/step - loss: 0.6372 - acc: 0.6441\n",
      "Epoch 9/55\n",
      "576/576 [==============================] - 0s 129us/step - loss: 0.6323 - acc: 0.6458\n",
      "Epoch 10/55\n",
      "576/576 [==============================] - 0s 119us/step - loss: 0.6272 - acc: 0.6562\n",
      "Epoch 11/55\n",
      "576/576 [==============================] - 0s 123us/step - loss: 0.6225 - acc: 0.6632\n",
      "Epoch 12/55\n",
      "576/576 [==============================] - 0s 134us/step - loss: 0.6181 - acc: 0.6649\n",
      "Epoch 13/55\n",
      "576/576 [==============================] - 0s 120us/step - loss: 0.6131 - acc: 0.6649\n",
      "Epoch 14/55\n",
      "576/576 [==============================] - 0s 112us/step - loss: 0.6069 - acc: 0.6788\n",
      "Epoch 15/55\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6028 - acc: 0.6788\n",
      "Epoch 16/55\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5962 - acc: 0.6927\n",
      "Epoch 17/55\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5908 - acc: 0.6927\n",
      "Epoch 18/55\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5872 - acc: 0.6962\n",
      "Epoch 19/55\n",
      "576/576 [==============================] - 0s 97us/step - loss: 0.5808 - acc: 0.6962\n",
      "Epoch 20/55\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5775 - acc: 0.7031\n",
      "Epoch 21/55\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5722 - acc: 0.7101\n",
      "Epoch 22/55\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.5660 - acc: 0.7188\n",
      "Epoch 23/55\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5614 - acc: 0.7066\n",
      "Epoch 24/55\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5570 - acc: 0.7101\n",
      "Epoch 25/55\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.5520 - acc: 0.7153\n",
      "Epoch 26/55\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5487 - acc: 0.7205\n",
      "Epoch 27/55\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5411 - acc: 0.7170\n",
      "Epoch 28/55\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5370 - acc: 0.7257\n",
      "Epoch 29/55\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.5320 - acc: 0.7222\n",
      "Epoch 30/55\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5257 - acc: 0.7448\n",
      "Epoch 31/55\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.5218 - acc: 0.7274\n",
      "Epoch 32/55\n",
      "576/576 [==============================] - 0s 92us/step - loss: 0.5176 - acc: 0.7326\n",
      "Epoch 33/55\n",
      "576/576 [==============================] - 0s 87us/step - loss: 0.5130 - acc: 0.7396\n",
      "Epoch 34/55\n",
      "576/576 [==============================] - 0s 90us/step - loss: 0.5088 - acc: 0.7465\n",
      "Epoch 35/55\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.5058 - acc: 0.7483\n",
      "Epoch 36/55\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.5043 - acc: 0.7431\n",
      "Epoch 37/55\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4981 - acc: 0.7569\n",
      "Epoch 38/55\n",
      "576/576 [==============================] - 0s 103us/step - loss: 0.4961 - acc: 0.7587\n",
      "Epoch 39/55\n",
      "576/576 [==============================] - 0s 112us/step - loss: 0.4937 - acc: 0.7639\n",
      "Epoch 40/55\n",
      "576/576 [==============================] - 0s 86us/step - loss: 0.4912 - acc: 0.7587\n",
      "Epoch 41/55\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4887 - acc: 0.7691\n",
      "Epoch 42/55\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4862 - acc: 0.7639\n",
      "Epoch 43/55\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4835 - acc: 0.7639\n",
      "Epoch 44/55\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4865 - acc: 0.7552\n",
      "Epoch 45/55\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4788 - acc: 0.7691\n",
      "Epoch 46/55\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4785 - acc: 0.7726\n",
      "Epoch 47/55\n",
      "576/576 [==============================] - 0s 78us/step - loss: 0.4797 - acc: 0.7708\n",
      "Epoch 48/55\n",
      "576/576 [==============================] - 0s 84us/step - loss: 0.4778 - acc: 0.7778\n",
      "Epoch 49/55\n",
      "576/576 [==============================] - 0s 80us/step - loss: 0.4813 - acc: 0.7691\n",
      "Epoch 50/55\n",
      "576/576 [==============================] - 0s 82us/step - loss: 0.4775 - acc: 0.7760\n",
      "Epoch 51/55\n",
      "576/576 [==============================] - 0s 85us/step - loss: 0.4729 - acc: 0.7778\n",
      "Epoch 52/55\n",
      "576/576 [==============================] - 0s 77us/step - loss: 0.4719 - acc: 0.7795\n",
      "Epoch 53/55\n",
      "576/576 [==============================] - 0s 79us/step - loss: 0.4700 - acc: 0.7778\n",
      "Epoch 54/55\n",
      "576/576 [==============================] - 0s 75us/step - loss: 0.4685 - acc: 0.7830\n",
      "Epoch 55/55\n",
      "576/576 [==============================] - 0s 74us/step - loss: 0.4680 - acc: 0.7778\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(X_train,y_train,epochs=55,batch_size=16)\n",
    "# 55 epochs: 78.125 / 77.43055555555556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 974,
     "status": "ok",
     "timestamp": 1541940345104,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "Fm1MSh3bPdS5",
    "outputId": "c546c93e-beaa-4850-aaa5-9b0f8c4246a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 [==============================] - 0s 274us/step\n"
     ]
    }
   ],
   "source": [
    "loss,acc = model2.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1541940346311,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "fcoQuFFQPnnT",
    "outputId": "7418ab49-4d24-4143-f129-0b5d5fa6d3fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.64583333333334"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1541940348564,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "1Vcg8OguQbAt",
    "outputId": "7aa4e815-cf96-4c8c-d0f2-9bca8d924204"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.99782079458237"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1541940349674,
     "user": {
      "displayName": "Syed Ibad Ul Hassan",
      "photoUrl": "https://lh4.googleusercontent.com/-r2T-C3N8dqI/AAAAAAAAAAI/AAAAAAAAA1w/rSe5ityyWKo/s64/photo.jpg",
      "userId": "17250535291094635562"
     },
     "user_tz": -300
    },
    "id": "l-mXrYdBRqh0",
    "outputId": "d56b5a4e-4290-4092-e579-d1a75d159cf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.29861111111111"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(history.history['acc']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDNjIzWOeacR"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEMXwonAf60C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PIMA Dataset.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
